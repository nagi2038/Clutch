{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67759967",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb4cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import os,re\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd1d68",
   "metadata": {},
   "source": [
    "# URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7441a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://clutch.co/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b82ede",
   "metadata": {},
   "source": [
    "# Saving File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb078451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = \"OutData\"+datetime.now().strftime('%Y-%m-%d %H%M%S')\n",
    "file = 'OutData'\n",
    "if not os.path.isdir(file):\n",
    "    os.mkdir(file)\n",
    "os.chdir(file)\n",
    "sep = '@@'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79551eaa",
   "metadata": {},
   "source": [
    "# Chrome properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd2f64b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "download_path = os.getcwd()\n",
    "prefs = {'download.default_directory':download_path}\n",
    "options.add_experimental_option('prefs', prefs)\n",
    "# ChromeDriverPath = r'../chromedriver.exe'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e126de3f",
   "metadata": {},
   "source": [
    "# Opening Chrome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb13d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd8914eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.maximize_window()\n",
    "driver.set_page_load_timeout(3)\n",
    "webActions = ActionChains(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84869449",
   "metadata": {},
   "source": [
    "# Setting Page load time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f69aafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.set_page_load_timeout(200000)\n",
    "driver.set_script_timeout(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab373a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1674b",
   "metadata": {},
   "source": [
    "# To get Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ae973dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBeautifulsoapData(xpath):\n",
    "    return BeautifulSoup(driver.execute_script(\"return arguments[0].innerHTML;\", driver.find_element(\"xpath\",xpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01b48c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = ' and '.join([i.strip(' ') for i in text.split('&')])\n",
    "    text = ' or '.join([i.strip(' ') for i in text.split('/')])\n",
    "    text = text.replace('\\t','')\n",
    "    text = ''.join(re.findall(r'\\b[a-zA-Z0-9_\\s]\\w+', text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f2378c",
   "metadata": {},
   "source": [
    "# Extracting URL of Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e6a5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = getBeautifulsoapData('/html/body/main/article/section[2]/div/div[2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a63e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store all Tech Domain URLs\n",
    "data_urls = {}\n",
    "\n",
    "for data in list_data.findAll('div'):\n",
    "    # To get sub Domain URLs\n",
    "    subDomains_urls = {}\n",
    "    \n",
    "    # Filtering out special chars\n",
    "    domains = cleanText(data.find('button').text)\n",
    "    \n",
    "    # Accessing links of each sub Domain\n",
    "    for sub_url in data.find('nav').findAll('a'):\n",
    "        # Adding all subDomain link to dict \n",
    "        subDomains_urls[cleanText(sub_url.text)] = urljoin(url,sub_url.get('href'))\n",
    "        \n",
    "    # Adding all links\n",
    "    data_urls[domains] = subDomains_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c820d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Tech Domains : ,Development,Design and Production,Marketing,Advertising,Business Services,IT Services\n"
     ]
    }
   ],
   "source": [
    "print(\"All Tech Domains : \", *data_urls.keys(),sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b38ec679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://clutch.co/directory/mobile-application-developers MobileApp Development\n",
      "clutch_Development_MobileApp Development.csv\n",
      "https://clutch.co/web-designers Web Design\n",
      "clutch_Design and Production_Web Design.csv\n",
      "https://clutch.co/agencies/digital-marketing Digital Marketing\n",
      "clutch_Marketing_Digital Marketing.csv\n",
      "https://clutch.co/agencies Advertising\n",
      "clutch_Advertising_Advertising.csv\n",
      "https://clutch.co/call-centers Call Centers\n",
      "clutch_Business Services_Call Centers.csv\n",
      "https://clutch.co/it-services IT Services\n",
      "clutch_IT Services_IT Services.csv\n"
     ]
    }
   ],
   "source": [
    "# sample Tech Domain links\n",
    "for outerDomain in data_urls.keys():\n",
    "    file_name = \"clutch_{}_{}.csv\"\n",
    "    dataUrls = data_urls[outerDomain]\n",
    "    for innerurl in dataUrls.keys():\n",
    "        print(dataUrls[innerurl], innerurl)\n",
    "        print(file_name.format(outerDomain, innerurl))\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e0fc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxPages()->int:\n",
    "    '''To get max pages for the technology'''\n",
    "    return int(getBeautifulsoapData('//*[@id=\"providers\"]/nav').findAll('li')[-1].find('a').get('data-page'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "844d1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextPage(SoupData,fileName ):\n",
    "    '''used to scrap data of page'''\n",
    "    with open(fileName, 'a',encoding='utf-8') as datewriter:\n",
    "        for d in SoupData.findAll( 'li', attrs = {'data-is-list':'true'}):\n",
    "\n",
    "            try:\n",
    "                postion = d.get('data-position')\n",
    "            except Exception as e:\n",
    "                postion = 'NA'\n",
    "            \n",
    "            # company name\n",
    "            try:\n",
    "                title = d.get('data-title') \n",
    "            except Exception as e:\n",
    "                title = 'NA'\n",
    "            \n",
    "            # website link\n",
    "            try:\n",
    "                website =  d.find('a', attrs = {'class':'website-link__item'}).get('href')\n",
    "            except Exception as e:\n",
    "                website = 'NA'\n",
    "            \n",
    "            # Location \n",
    "            try:\n",
    "                location = d.find('span' , attrs = {'class':'locality'}).text\n",
    "            except Exception as e:\n",
    "                location = 'NA'\n",
    "                \n",
    "            # rating of company\n",
    "            try:\n",
    "                rating =  str(float(d.find('span', attrs = {'class':'rating sg-rating__number'}).text))\n",
    "            except Exception as e:\n",
    "                rating = \"NA\"\n",
    "            \n",
    "            # reviews count\n",
    "            try:\n",
    "                reviews = d.find('a' ,attrs = {'data-link_text':\"Reviews Count\"}).text.replace('\\n','').replace('\\r', '').replace('\\t', '').strip(' ')\n",
    "            except Exception as e:\n",
    "                reviews = 'NA'\n",
    "            \n",
    "            # Hourly Rate\n",
    "            try:\n",
    "                hourlyRate = d.findAll('div', attrs = {'class' : 'list-item custom_popover'})[0].find('span').text.strip(' ')\n",
    "            except Exception as e:\n",
    "                hourlyRate = 'NA'\n",
    "                \n",
    "            # Mini Project size \n",
    "            try:\n",
    "                miniProjSize = d.find('div',attrs = {'class' : 'list-item block_tag custom_popover'}).find('span').text\n",
    "            except Exception as e:\n",
    "                miniProjSize = 'NA'\n",
    "            \n",
    "            \n",
    "            # Employee size \n",
    "            try:\n",
    "                empSize = d.findAll('div' , attrs = {'class':'list-item custom_popover'})[1].find('span').text\n",
    "            except Exception as e:\n",
    "                empSize = 'NA'\n",
    "                \n",
    "            \n",
    "            out = sep.join([postion,title,website,location,rating,reviews,hourlyRate,miniProjSize,empSize])+'\\n'\n",
    "            datewriter.write(out)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64a15b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinkData(domainUrl, fileName):\n",
    "    # Deleting all cookies to avoid varification for robot\n",
    "    driver.delete_all_cookies()\n",
    "    \n",
    "    # Each Tech domain url\n",
    "    driver.get(domainUrl)\n",
    "    \n",
    "    # Max pages in each Tech domain\n",
    "    maxPages = min(20,getMaxPages())\n",
    "    \n",
    "    \n",
    "    print(\"looping over Pages : \",maxPages)\n",
    "    print(\"Started scrapy for :\" ,domainUrl)\n",
    "    \n",
    "    # Looping over all pages in Tech Domain.\n",
    "    for i in range(maxPages+1):\n",
    "        \n",
    "        driver.delete_all_cookies()\n",
    "        print(i,end = ',')\n",
    "        \n",
    "        # generating url for next page \n",
    "        driver.get(urljoin(domainUrl,'?page={}'.format(i)))\n",
    "        \n",
    "        # gathering data of Each page\n",
    "        SoupData = getBeautifulsoapData('//*[@id=\"providers\"]/div[2]')\n",
    "        \n",
    "        # passing it to next page function to get requried data from from that page\n",
    "        nextPage(SoupData,fileName)\n",
    "        \n",
    "    print(\"\\nEnded scrapy for :\" ,domainUrl)\n",
    "    print('\\n\\n')\n",
    "#         break\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "727ce43a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For scrapying Entire Data'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''For scrapying Entire Data'''\n",
    "for outerDomain in data_urls.keys():\n",
    "\n",
    "    # Gathering all subDomain links\n",
    "    dataUrls = data_urls[outerDomain]\n",
    "    for innerurl in dataUrls.keys():\n",
    "        \n",
    "        # Generating file according to domain and subdomain\n",
    "        file_name = \"clutch_{}_{}.tsv\"\n",
    "        file_name = file_name.format(outerDomain, innerurl)\n",
    "        \n",
    "        \n",
    "        print(dataUrls[innerurl], innerurl)\n",
    "        print(os.path.join(os.getcwd(),file_name))\n",
    "        \n",
    "        # writing columns to a file\n",
    "        columns = ['Postion', 'title','website','location','rating','reviews','hourlyRate','miniProjSize','empSize']\n",
    "        with open(file_name, 'w') as w:\n",
    "            w.write(sep.join(columns)+'\\n')\n",
    "            \n",
    "        # to get Data of subDomain\n",
    "        getLinkData(dataUrls[innerurl], file_name)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e042bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_tsv = [i for i in os.listdir() if i.endswith('.tsv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53c7ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting files to xlsx format.\n",
    "for i in files_tsv:\n",
    "    xlsx_name = i.replace('.tsv', '.xlsx')\n",
    "    try:\n",
    "        pd.read_csv(i,sep = sep,engine='python').to_excel(xlsx_name, engine=\"xlsxwriter\", index = False)\n",
    "        os.remove(i)\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fc52aa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current URL : https://clutch.co/agencies/app-marketing\n",
      "E:\\PythonProjects\\webScrapping\\Clutch\\OutDataLinks\\clutch_Marketing_Mobile Marketing.tsv\n",
      "looping over Pages :  20\n",
      "Started scrapy for : https://clutch.co/agencies/app-marketing\n",
      "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,\n",
      "Ended scrapy for : https://clutch.co/agencies/app-marketing\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' If any issue occured we can scrap data from individual Tech Domains (Optional)'''\n",
    "# OutteDomain = 'Marketing'\n",
    "# outter = data_urls[OutteDomain]\n",
    "# for innerurl in outter.keys():\n",
    "    \n",
    "    \n",
    "#     file_name = \"clutch_{}_{}.tsv\"\n",
    "#     file_name = file_name.format(OutteDomain, innerurl)\n",
    "    \n",
    "#     print('Current URL :' ,outter[innerurl])\n",
    "#     print(os.path.join(os.getcwd(),file_name))\n",
    "    \n",
    "#     columns = ['Postion', 'title','website','location','rating','reviews','hourlyRate','miniProjSize','empSize']\n",
    "#     with open(file_name, 'w') as w:\n",
    "#         w.write(sep.join(columns)+'\\n')\n",
    "#     getLinkData(outter[innerurl], file_name)\n",
    "# #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a724e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_urls['Marketing'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "626e313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_urls['Marketing'] = {'Mobile Marketing': 'https://clutch.co/agencies/app-marketing'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d042e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_urls['Marketing'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2060a4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792db5d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
